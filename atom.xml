<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[blooming bridges gradually graduating]]></title>
  <link href="http://bloomingbridges.github.com/Sammelsurium/atom.xml" rel="self"/>
  <link href="http://bloomingbridges.github.com/Sammelsurium/"/>
  <updated>2012-12-26T18:01:08+00:00</updated>
  <id>http://bloomingbridges.github.com/Sammelsurium/</id>
  <author>
    <name><![CDATA[Florian Brueckner]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[INHABIT]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/inhabit/"/>
    <updated>2012-12-14T21:15:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/inhabit</id>
    <content type="html"><![CDATA[<p>One of the benefits of the close partnership between the course and <em>IBM</em> is their <a href="http://www.ibm.com/smarterplanet/uk">Smarter Planet</a> workshop they hold for us final year students once a year. It&#8217;s a chance to have your idea evaluated by an industry giant whose initiative is to make the world a better place. What&#8217;s not to like? Probably the fact that every idea you will come up with during the workshop is either already being done or will <a href="http://moorescloud.com">hit the market soon</a>.</p>

<p>Luckily, we identified a series of issues within our own household that could be easily resolved through smartification. What we aim to improve is the relationship between friends or strangers living together by minimising the window for confrontation and making their everyday more efficient / convenient. Here are some example scenarios:</p>

<ul>
<li><p>Let&#8217;s say I&#8217;m returning home and I wasn&#8217;t aware that my housemate dead-locked the door when he left the house earlier so I stand there on the doorstep looking rather confused for a minute. <em>What if the house would automatically double-lock the door after the last person has left and unlock for me as I approach it?</em></p></li>
<li><p>Let&#8217;s say I&#8217;m at university and I&#8217;m expecting a delivery. <em>If only I could tell whether any of my housemates are home that afternoon to receive the package, so I could send them a notice. I don&#8217;t have their phone numbers? No problem, the system would take care of that.</em></p></li>
<li><p>One big cause for quarrel in shared accommodation is the thermostat, as throughout the day inhabitants will adjust it according to their preference. <em>How about the house would adjust the heating automatically, keeping the room temperature at a level that suits everyone?</em></p></li>
<li><p>Assuming you have a housemate who sometimes falls asleep watching films on a volume that makes it hard for you to work or sleep next door. <em>That&#8217;s a no-brainer, just have the house turn off the television once it senses the housemate has fallen asleep! Likewise if I wanted to listen to music, ignoring the fact that other inhabitants might be asleep the system could inform me of my inconsiderate  behaviour.</em></p></li>
<li><p>It was housemate B&#8217;s turn to take the bins out today, but apparently he&#8217;s either forgot or doesn&#8217;t care. <em>If the system would remind him or her in the morning and display this information somehwere for everybody to see he or she might feel more inclined to do it.</em></p></li>
<li><p>If I was going to make tea or coffee and quickly wanted to ask around the house whether anybody would want some. <em>This could happen via a single button press on my phone.</em></p></li>
<li><p>At the moment we all do our shopping individually, whereas we could save a considerate amount of money buying in bulk and ordering groceries online. <em>A central shopping list, which was aware of who put what item on would be able to work out the invidual bills in no time and would even be able to stock up on common used items like toilet paper before they run out.</em></p></li>
</ul>


<p>This can all be done using a couple of sensors (RFID, heat, movement), web technologies and geolocation or RFID keyrings. We call our system <strong>INHABIT</strong>. It will live on a central server in your house and is exposed via a myriad of interfaces: an app on the individual&#8217;s phone, a display mounted in the hallway or maybe the television set in the living room.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/monitor_screenshot.png" alt="INHABITor screenshot" /></p>

<p>And it doesn&#8217;t end here. There is lots of scope for additions due to the modular nature of the system. One could, for instance, choose to share one&#8217;s public calendar with <strong>INHABIT</strong>, so it can make better predictions on what time you&#8217;ll be home.</p>

<p>Naturally there will be people, who are concerned about their privacy and would rather not share their exact whereabouts with acquaintances her or she happens to live with. This is why we obfuscate sensitive data and provide  individual privacy control settings.</p>

<p>For our prototype we chose to implement presence using <em>RFID</em> keychains, the individual&#8217;s newsfeeds (using the <em>MQTT</em> protocol to send notifications to and from a <em>Node</em> server, communicating with the mobile web app and the monitor website) and a simple intelligence that would adjust the heating depending on who is home, inform you of sleeping housemates when you enter through the door and allow you to send tea calls around.</p>

<iframe width="700" height="394" src="http://www.youtube.com/embed/alJG27F6YG0?rel=0" frameborder="0" allowfullscreen></iframe>


<p>For a one-week project it&#8217;s been quite ambitious and with so many possibilities at hand it can be difficult to tell where one should draw the line. Turns out we did just the right amount as <strong>INHABIT</strong> was voted the best project and won us each a bag of <em>IBM</em>-branded stuff.</p>

<p>So far it&#8217;s been the most seamless group work we&#8217;ve done for this module as it was really simple to divide up tasks. I must also say that I thoroughly enjoyed working on something genuinely useful for a change.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Persistence of Memory]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/the-persistence-of-memory/"/>
    <updated>2012-12-03T19:33:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/the-persistence-of-memory</id>
    <content type="html"><![CDATA[<p>&#8220;We have the technology, so we might as well use it&#8221;, seems to have been the theme for our last <em>DAT301</em> micro project. Equipped with a kinect device, our group decided after long debate that we should try and recreate famous landscapes by 3D scanning household appliances and food. So here goes the first of a series: Salvador Dalí&#8217;s Surrealist painting &#8220;The Persistence of Memory&#8221;, or, &#8220;that one with the melting clocks&#8221;:</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/dali_final.png" alt="The Persistence of Memory" /></p>

<p>In my naivety I assumed that it would be fairly simple to mash the scanned meshes together and make some quick refinements. However, the level of detail on these was way higher than expected and the whole scene became  unmanageable rather early in the composition process. Texturing would therefore also have been a nightmare so I stopped after putting on materials.</p>

<p>I tried cleaning up and to eventually reduce the amount of detail using <em>Meshlab</em>, but it didn&#8217;t seem much use either as it kept crashing whenever one deletec vertices from the scene. So in the end a couple of objects like the tree and the ground are actually created via traditional modelling.</p>

<p>It&#8217;s been a fun experiment nevertheless and I believe we captured the essence of the landscape fairly well for a one-day project. Here&#8217;s the original one more time for your reference:</p>

<p><img src="http://3.bp.blogspot.com/_qsSkaXuuUE8/TT6hVjc-L9I/AAAAAAAAAD4/1JnIshd9pCw/s1600/the_persistence_of_memory_-_1931_salvador_dali.jpg" alt="Original" /></p>

<p>Originally my aim was to have the painting explorable in stereoscopic 3D, the time-constraints however made this impossible. Also I think I like the idea of simply re-creating artwork more ever since I stumbled upon this <a href="http://recodeproject.com">computer art conservation project</a>. Apart from being a &#8220;copy&#8221; in a wider sense, this project was more an exploration of the polygonal aesthetic, so a less detailed art style such as expressionism tends to lend itself better than let&#8217;s say a photorealistic image of <em>Times Square</em>. And of course similar things have been done <a href="http://www.thecreatorsproject.com/blog/when-indiana-jones-met-dali-classic-paintings-become-8-bit-game-mashups">before</a>.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/indy_persistence.png" alt="Indiana Jones and the Persistence of Memory" />
&copy; <em>thecreatorsproject</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Homebrew 3D scanning and motion capturing]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/homebrew-3d-scanning-and-motion-capturing/"/>
    <updated>2012-12-03T19:32:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/homebrew-3d-scanning-and-motion-capturing</id>
    <content type="html"><![CDATA[<p>Today Mister <em>Musaab Garghouti</em> from the local company <a href="http://www.real-visual.com">Real Visual</a> came into our <em>Real-time</em> session to show us some of the projects he&#8217;s been involved with and to let us have a play with some rather cumbersome-looking hardware.</p>

<!-- ![MoCap session](https://dl.dropbox.com/u/998319/DAT/mocap.jpg) -->


<p>Musaab&#8217;s focus has very much been on 3D scanning and printing, so he&#8217;s given us a brief rundown of the most commonly used practices used in the industry, then went on to demonstrate how a consumer device such as <em>Microsoft&#8217;s</em> <em>Kinect</em> can be misused for scanning real-world objects and organisms such as myself.</p>

<p>What makes this possible is the way the <em>Kinect</em> camera works: On top of the video stream it receives through the lens, an infrared sensor is constantly scanning the field of vision for depth information. Thus you&#8217;re able to &#8220;feel&#8221; the surfaces by taking the device and moving it slowly around the object. The sensor is able to pick up details right down to a centimeter&#8217;s resolution, which is more than enough for what it was originally built to do.</p>

<p>As you can imagine it doesn&#8217;t work very well on see-through or reflective materials in general. The software we used to interface with the device to see if we can get it to produce us some motion capture data seemed quite temperamental, but that&#8217;s porbably down to the hacking nature of the exercise. I know too well that this sort of work requires a lot of stamina as you do need to re-do things over and over again.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/kinectme.png" alt="Kinect me" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Delivery Day Horrors]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/delivery-day-horrors/"/>
    <updated>2012-12-03T12:06:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/delivery-day-horrors</id>
    <content type="html"><![CDATA[<p>When pondering what is wrong with this world and how we can improve on it using technology, there is one offender that keeps crossing my mind: parcel delivery services.</p>

<p>Only yesterday I had the displeasure of waiting all day for a parcel to be re-delivered as I didn&#8217;t receive any notice that they&#8217;d knock at my door one day early. No indication as to when they&#8217;ll stop by the next day, the tracking site on the internet only read &#8220;sometime between 9am and 5pm&#8221; and indeed, I opened the door to receive my delivery at 4.58pm. It doesn&#8217;t have to be this way.</p>

<p>So what if we installed parcel-sized containers on our doorsteps, which would unlock for delivery men when we&#8217;re out and about? They wouldn&#8217;t have wasted time and resources to be disappointed at my doorstep and I wouldn&#8217;t have to be home the next day, frantically refreshing the tracking website: win win.</p>

<p>If you just read this and thought &#8220;Well son, your problem is already a thing of the past! I&#8217;ve been using <a href="https://www.bufferbox.com">Service X</a> for many years&#8221;, then consider yourself lucky to be living somewhere that is actually worth living in and not the miserable South West of England.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/bufferbox.jpg" alt="Bufferbox" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[UbiComp Seminar afterthoughts]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/ubicomp-seminar-afterthoughts/"/>
    <updated>2012-11-28T22:44:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/ubicomp-seminar-afterthoughts</id>
    <content type="html"><![CDATA[<p><img src="https://dl.dropbox.com/u/998319/DAT/future.jpg" alt="Microsoft's vision of the future" /></p>

<p>In our second <em>Everyware</em> seminar this year we discussed the merits of <em>ubiquitous computing</em> (short: <em>ubicomp</em>), a paradigm by which our interactions with computers become more and more integrated into the human experience as they are no longer restricted to our desk. Everyday objects such as chairs and wardrobes will soon have computing power embedded, they will make choices for us so we don&#8217;t have to and of course they will all be able to communicate with each other.</p>

<p>But is it really a good idea to give all our agency away to a machine? There are still some things left that can be done way more efficiently by humans, this includes all creative processes. Take a camera roll for example. A computer might be able to judge pictures based on their blurriness or even know about your taste, but in the end only you can decide which pictures to keep and which ones to throw out. No room for automation there.</p>

<p>Somehow we managed to steer the conversation towards user interface design and how good a job <a href="http://www.realmacsoftware.com/clear/">Clear</a> was doing at getting rid buttons and icons in their interface (if only I had finished my list making app in time..). I agree that removing clutter in software will help the user focus on a particular task, but removing the interface from everyday activities such as getting rid of light switches is, at least in my opinion, too huge a sacrifice in terms of control. Then again I&#8217;m usually not very open to change and I will continue to insist that voice control, even if it worked perfectly, would always be conceptually flawed.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scaffolding]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/scaffolding/"/>
    <updated>2012-11-26T23:05:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/scaffolding</id>
    <content type="html"><![CDATA[<p>The following is an attempt come up with a rudimentary structure for my dissertation. At this point I am still not exactly sure what my argument is going to be as there is still a lot of research to be undertaken. Another growing concern is that analysing other designers&#8217; work with a scientific eye might not be considered academic enough. Judge for yourself and feel free to chip in for some direction.</p>

<h2>The Expressiveness of the Pixel (working title) - Dissertation structure draft</h2>

<h3>Introduction</h3>

<ul>
<li>I am going to talk about how to make games that reach out of the screen and stay in people&#8217;s heads. The games analysed in this dissertation are made by a single person, so the techniques uncovered can be used by (and apply to) big studios alike.</li>
</ul>


<h3>Chapter 1 - The moving image versus the Interactive experience</h3>

<ul>
<li>There&#8217;s a trend among games with a high-production value to borrow elements from cinema in order to make them more entertaining (e.g. Uncharted series), whereas indiependent developers have shown that you can make more &#8216;meaningful&#8217; games with limited resources and even lack of talent.</li>
</ul>


<h3>Chapter 2 - Less is More</h3>

<ul>
<li><p>I pick three exemplary titles, describe what they do and why the work so well based on scientific evidence from the field of psychology</p></li>
<li><p>Thirty Flights of Loving by Brendon Chung</p>

<ul>
<li>&#8220;Thirty Flights of Loving is about 15 minutes long, as minimalist in its visuals as it is in its storytelling, and drops you straight into a linear, first-person narrative about love, betrayal and grand heists. It progresses at breakneck pace, bombarding you with imagery and incidental detail that might fly straight over your head the first time through. It tells a story almost entirely without words, written or spoken; this is the videogame equivalent of punchy, avant-garde short fiction.&#8221;</li>
</ul>
</li>
<li><p>Don&#8217;t Look Back - Terry Cavanagh</p>

<ul>
<li>A platformer that relies on a three colour palette</li>
</ul>
</li>
<li><p>Thomas Was Alone by Mike Bithell VS. Alphaland by Jonas Kyratzes</p>

<ul>
<li>All characters are just blocks, the ultimate reduction</li>
</ul>
</li>
</ul>


<h3>Chapter 3 - Just the right Amount of Ambiguity</h3>

<ul>
<li>Critical discussion where I compare the effectiveness of the above mentioned to equivalent commercial ones (i.e. Super Crate Box vs. Left 4 Dead) in terms of visual story-telling</li>
<li>Debate whether an older generation is more likely to engage with &#8220;pixel art&#8221; due to nostalgia</li>
</ul>


<h3>Chapter 4 - The Attraction of the Unknown</h3>

<ul>
<li>Here I outline more ways of achieving similar sentiments (narrative, sound, co-op)</li>
<li>IN//CUBUS - Discussion of the techniques used in my Final Year project</li>
</ul>


<h3>Chapter 5 - Conclusion: Do try this at home</h3>

<ul>
<li>But be aware that the methods outlined do not guarantee success</li>
<li>We need more empathy in games</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unsticking the Middle]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/unsticking-the-middle/"/>
    <updated>2012-11-23T20:54:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/unsticking-the-middle</id>
    <content type="html"><![CDATA[<p>For our second collaboration with the school of architecture we were asked to create something novel that would attract visitors to the ex-mining town St. Blazey, Cornwall. The people of St. Blazey feel a little overshadowed by the surrounding &#8220;metropoles&#8221; St. Austell and Fowey, something the council wants to change by building a museum on the former R&amp;D area of the clay mining grounds, the design of which is - coincidentally - carried out by the architecture students of <em>Plymouth University</em>.</p>

<p>Early on in the project we decided to tackle the social issues that St. Blazey is dealing with, the feeling of disconnectedness and the generation gap. Saul and myself felt strongly that we should make an installation, which would blend into the environment and which would only reveal its interaction by walking past / <a href="http://www.artcom.de/en/projects/project/detail/duality/">over it</a>.</p>

<p>A great way of doing so - we thought - was have people accidentally create harmonic music together by giving them an instrument that doesn&#8217;t require any mastery. Heavily inspired by Justin Windle&#8217;s <a href="http://soulwire.co.uk/experiments/particle-node-sequencer/">Particle Node Sequencer</a> I imagined glowing poles of light in a medium-sized dark room that play back notes whenever people gather around them holding hands, to reference the strong sense of purpose the miners had felt back in the day risking their lives down in the shafts of the St. Blazey clay mines.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/particle_node_sequencer.png" alt="Justin Windle's Particle Node Sequencer" /></p>

<p>A less extravagant, yet more portable solution was to buid a monome or <a href="http://blog.iso50.com/23116/start-making-some-music-right-now/">tone matrix</a> out of illuminated tiles on the floor. To hit two birds with one stone we thought we could have multiple installations, one in each of the neighbouring towns that would allow for remote collaboration, each location being identifiable through their unique colour.</p>

<p>One building in particular within the museum had been recommended to us for the main attraction, a room called the archive. To do the location justice we thought about having a separate room which would house a physical representation of the tunes created via the monomes. After some serious Health &amp; Safety considerations we thought it would be nice to have a transforming floor made up of many little cubes on pistons, covered by a stretchy material.</p>

<p>One week was not enough to create a physical prototype with Arduinos, even going for the smallest matrix possible there was still too much room for error. Hence I suggested to make a virtual prototype using <em>Unity3D</em>, whilst the others would be looking into creating a web version and synthesising music  using the <em>Web Audio API</em>. Getting them to talk to each other resulted in the usual mess of <em>NodeJS</em>, websockets and OSC.</p>

<!-- ![Unity screenshot](https://dl.dropbox.com/u/998319/DAT/moving_floor_unity.png) -->




<iframe width="700" height="394" src="http://www.youtube.com/embed/2gUvwj4tp0A?rel=0" frameborder="0" allowfullscreen></iframe>


<p><em>demo video, courtesy of Benjamin Ashman</em>; annoying glare, courtesy of whiteboard paint™</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/monome_presentation.jpeg" alt="presentation" /></p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/moving_floor.jpeg" alt="Moving floor model" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Post-workshop répondaire]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/post-workshop-repondaire/"/>
    <updated>2012-11-23T12:41:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/post-workshop-repondaire</id>
    <content type="html"><![CDATA[<h3>Project 1b workshop: 5/10/12 – 19/10/12</h3>

<p><strong>a) Explain three things which you have learnt from this workshop.</strong></p>

<ul>
<li><p>The way architecture is taught is much more free-form (open workspaces, constant tutoring) and even in a field so specific you still have the choice to do whatever you want.</p></li>
<li><p>Architects care a lot about materials and finding the right texture for things whereas we DAT students seem to be more concerned with the invisible.</p></li>
<li><p>Their conceptual nature gets in the way of our nature of building things. You cannot force collaboration onto people, it has to happen naturally for mutual reasons.</p></li>
</ul>


<p><strong>b) What are three things which could have been better/improved upon?</strong></p>

<ul>
<li><p>The briefs could have been less wordy and more concise, especially in places where they differed for groups and even team members.</p></li>
<li><p>The workshops could have been better integrated into each other&#8217;s curriculum. For the most time it felt as if our work was just sugar-coating and not very fundamental as the architects already had a head start on designing the buildings. This mismatch of expectations plus the fact that we got assessed on different presentations lead to a massive loss of interest in either side&#8217;s work.</p></li>
<li><p>Some people might benefit from an introduction into basic computer skills.</p></li>
</ul>


<p><strong>c) Did you find the project challenging? Explain if so and why.</strong></p>

<p>Yes, I did find the projects challenging, however made unnecessarily challenging by the vagueness of the brief given. Too much time was wasted re-evaluating our ideas against the marking criteria and the relevance to the module content.</p>

<p><strong>d) After the workshop experience, how would you define a site?</strong></p>

<p>A site is more than just a location on a map. It is a self-contained space, always in transition and a playground for artists. Any site benefits from meaning that it receives through contacts with humans.</p>

<p><strong>e) What is your now current understanding of ubiquitous computing?</strong></p>

<p>See my post <a href="http://bloomingbridges.github.com/Sammelsurium/ubicomp-seminar-afterthoughts/">UbiComp Seminar afterthoughts</a></p>

<p><strong>f) How did you engage your course skills to the task? (field of DAT or Architecture)</strong></p>

<p>I picked up a lot of knowledge on computer graphics and more specifically <em>Unity3D</em> in a short amount of time for the prototype. Figuring out how to make two things talk to each other that otherwise would not (Arduino and NodeJS, NodeJS and Processing/Unity) under pressure was key and it took some exerience to be able to tell what would and what wouldn&#8217;t work from a technical perspective.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A taste of Unity]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/a-taste-of-unity/"/>
    <updated>2012-11-13T22:50:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/a-taste-of-unity</id>
    <content type="html"><![CDATA[<p>In anticipation of the upcoming <em>DAT301</em> workshop in immersive 3D and the variety of use cases for other assignments  that keep forming in front on my inner eye I started looking into the popular Danish game engine by the name of <em>Unity3D</em>.</p>

<p>When you open up the application for the first time you&#8217;re greeted by a fairly complex demo project (in the case of version 3.5b, <em>Angry Bots</em>) nested within an array of hideous-looking grey panels. On top of those floats a window that claims to hold all the information one needs to get up and running. Granted, moving around in the 3D scene view works like a charm (using 3Ds Max-style hotkeys Q,W,E,R for navigating, translating, rotating and scaling) and is way more intuitive than other 3D packages that I had the displeasure to work with lately.</p>

<p>Then came shock number two: No coding tutorial? You&#8217;re not even going to tell me how to rotate a cube around itself? Panic-stricken I clicked around the interface looking for clues. It appeared as if the general workflow of a unity developer consisted solely of buying models from the <em>Asset Store</em> and placing them into a scene. This certainly wasn&#8217;t a very pleasing thought and I refused to accept it as true since I have watched the <a href="http://www.indiebuskers.net">indiebuskers</a> make games with <em>Unity</em>, spending most of their time in MonoDevelop or Visual Studio, there must be another way..</p>

<p>When I happened to fall ill over the weekend I spent a whole day just watching all the <em>Unity</em> video tutorials I could find on <em>YouTube</em> from the comfort of my bed. Filled to the brim with new knowledge I gave it another shot and made my model of a walrus head rotate on top of a particle system. Sweet!</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/mondrian.png" alt="Mondrian le Morse" /></p>

<p>Importing models including textures was surprisingly pain-free. My weapon of choice was C#, although I noticed that <em>Unity</em>&#8217;s implementation of <em>JavaScript</em> seems to be more closely related to the <em>ActionScript</em> standard due to the use of static typing, which I like. Long time <em>Flash</em> devs even reported a similarity in their workflow which I can see to an extent. The way it works is you import your assets and attach so called &#8220;behaviours&#8221; to them, little scripts that each contain their own Init and Update loop (yes, all function names a capitalised..). You can save your script-laden asset as a &#8220;prefab&#8221; (think <em>MovieClip</em>), which you can then drag into the scene or instantiate through another script.</p>

<p>What took some time to adjust to, however, was that everything is based on vectors (i.e. you can&#8217;t just move an object two units to the left, but have to provide a new vector for its position), but that&#8217;s only a small sacrifice compared to learning WebGL and doing everything without an instant representation. This is actually really nice, once you pressed the play button you can inspect all the elements in your scene and make adjustments. This makes debugging a breeze.</p>

<p>Now to try something more challenging. How about I try to use it with a wiimote? Nope, you need the Pro version for that. Kinect? Pro version required. Hmm.. Shadows? Pro version! Can I at least get the dark interface skin? Sorry, no. So I went and acquired a student licence from the official reseller for the upcoming and much appraised version 4, which luckily only turned out to be around £90 rather than the full price of £1500.</p>

<p>In the end I regained my faith in the software (albeit the fact that it wouldn&#8217;t let me open the new version due to popular demand) and you may expect a lot more 3D work from me in the near future (or at least until my licence runs out).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Ecologies Symposium]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/data-ecologies-symposium/"/>
    <updated>2012-11-10T13:47:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/data-ecologies-symposium</id>
    <content type="html"><![CDATA[<p><img src="https://dl.dropbox.com/u/998319/DAT/dataecologies.jpg" alt="Data Ecologies poster" /></p>

<p>Today I attended my first ever symposium, organised by the <em>Institute for Digital Art &amp; Technology</em> and conveniently taking place on campus. To be fair I only went as our <em>DAT302</em> group was asked to present our <em>SixthSense</em> technology that we produced for the <em>Devonport High School for Boys</em>, the workload simply wouldn&#8217;t have allowed for a weekend off otherwise.</p>

<p>My favourite pair of speakers were Alice Sharp and her assistant from <a href="http://invisibledust.com">Invisible Dust</a>, who are very much concerned with making invisible things visible and accentuating environmental issues. I find their noble approach to collaboration quite fascinating, definitely a useful contact to have I imagine.</p>

<p>One of their <a href="http://invisibledust.com/project/jeremy-deller-tracking-bats-in-2012/">projects</a> which they talked in great length about dealt with the social recording of bats using an iOS app in conjunction with a special microphone. It made <a href="http://benashman.co.uk/p/birdwire/">Birdwire</a> look rather static and boring in comparison.</p>

<p>The most inspiring talk given was given by the <a href="http://www.owlproject.com">Owl Project</a>, a group of artists who from making musical instruments out of logs for personal use to having a gigantic watermill / musical instrument  built and placed on the Tyne river. I urge you to check out <a href="http://www.flowmill.org">~Flow</a>.</p>

<p>For a short while it even made me consider creating a programmable music box of my own. Then reality kicked in and we found ourselves in Babbage, setting up our thing. I&#8217;ve met Ziad Ewais who had some interesting things to say about <em>Unity</em> development. If you happened to be around you would have seen the usual DAT projects. Saying that I have no measure of success other than perhaps the amount of people who followed the symposium online or the quality of the food.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/sixthsenseminusjim.jpg" alt="Group photo" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FYP: Technologies]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/fyp-technologies-used/"/>
    <updated>2012-11-09T02:54:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/fyp-technologies-used</id>
    <content type="html"><![CDATA[<p>The technology aspect of the <em>IN//CUBUS</em> project regards twofold: the hardware that is reponsible for the visual feedback and the software running on both the mobile clients and the server running on a machine small enough to fit into the sculpture (<em>Raspberry Pi</em> or perhaps borrow a <em>Mac Mini</em>?).</p>

<h3>Electronics</h3>

<p>For the grabbing your attention part I was thinking at least two bright LEDs, one white (for general ominous glowing) and a red one for feedback. For the physical map I will most likely buy a pre-built LED matrix, for it gives me constraints in terms of how many rooms I have to manage per session. Unfortunately they never seem to come with blue LEDs and if I wanted the sculpture to be any bigger I&#8217;d have to get them in a power of two..</p>

<p>Furthermore will I need a handful of proximity sensors and or pressure pads to check for nearby gallery visitors. The electronics will be powered by my trusted <em>Arduino Uno</em> to keep the cost down.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/FYP/ledmatrixfinished.jpg" alt="led matrix" /></p>

<h3>Enclosure</h3>

<p>For the enclosure I will need a semi-seethrough material like frosted glass or if I really can&#8217;t afford it tracing paper. Here&#8217;s a guide for constructing a <a href="http://www.instructables.com/id/LED-Cube-Night-Light/">LED cube night light</a> over at instructables.com using scratched acryllic sheets and a single LED and glue.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/FYP/FSRRUXDG5FR5LWG.LARGE.jpg" alt="LED Night Light" /></p>

<h3>Enchantment</h3>

<p>On the software side of things we&#8217;ll have the usual combination of <em>Node</em>, <em>JavaScript</em>, <em>WebSockets</em> and either <em>HTML5 Canvas</em> or <em>Flash</em> depending on whether I get either enough demo devices or too fed up with trying to set up a Linux wifi hotspot.</p>

<p>If worse comes to worst I can always grab a copy of <em>Mozilla</em>&#8217;s browser MMORPG experiment <em>BrowserQuest</em> and modify it according to my needs.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/FYP/promo-title.jpg" alt="BrowserQuest" /></p>

<p>Play <a href="http://browserquest.mozilla.org">Mozilla&#8217;s BrowserQuest</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The democratisation of TV privileges]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/the-democratisation-of-tv-privileges/"/>
    <updated>2012-11-03T13:27:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/the-democratisation-of-tv-privileges</id>
    <content type="html"><![CDATA[<h3>Inception</h3>

<p>When we were asked to create a piece of media whose playback was somehow influenced by an <em>RSS</em> feed for our second mini project, the initial collective response was something along the lines of &#8220;Not again!&#8221;. To make matters worse, Dr. Lock provided us with a code example, which only needed two minor changes in order for the weakest of students to create something &#8220;acceptable&#8221;. Where&#8217;s the fun in that?</p>

<p>So I teamed up with <a href="https://twitter.com/imaginehonesty">Lizzie Seymour</a> as we shared the sentiment that feeds didn&#8217;t do the module name much justice. Still within the workshop it crystallised that we wanted to experiment with audience participation, to hand the control over to our spectators as in pieces like <a href="http://seb.ly/work/pixelphones/">PixelPhones</a> by Seb Lee-Delisle. For demonstration purposes we&#8217;d stitch together a series of cute kitten videos and make everyone in the room rub their phone screens in order to &#8220;upvote&#8221; their favourite clip.</p>

<p>We tried several JS libraries for picking up gestures in mobile browsers, but couldn&#8217;t find one that picked up more than one finger swipes on the <em>Android</em> default browser reliably. When Simon expressed his growing concern that we&#8217;d get too close to the upcoming assignment in taking the feed out of the equation, we came up with a slightly different scenario: We&#8217;d take the largest source of free video material on the internet available (<em>YouTube</em>), project it onto a wall and gave everyone in the room access to the same remote control. Thus, <em>The democratisation of TV privileges</em> was born.</p>

<h3>Development</h3>

<p>I feared that if we wanted to go down the <em>Processing</em> route I&#8217;d have to write a script that scraped <em>YouTube</em> for videos and downloaded them in advance. That would have been neither feasible nor very in lign with copyright law, so we decided to stick with web technologies for this one. Luckily <em>YouTube</em>&#8217;s recent update to the Player API allows you to control embedded videos via <em>JavaScript</em>. The original purpose of the <em>chromeless player</em> was to allow for customisation of the controls, little did they know it would make my life much much easier.</p>

<p>Lizzie went off to write the backend, which would generate <em>YouTube</em> playlist for search terms and provide them in human-readable XML. As for the interactive part, I used this project as an excuse to dig deeper into <a href="http://nodejs.org">NodeJS</a> and their implementation of the WebSocket API as I felt this was too big a thing not to have in your toolbelt these days (as our projects in other modules so splendidly demonstrated). I used <a href="http://nodejitsu.com">nodejitsu.com</a> for hosting and <a href="http://socket.io">socket.io</a> for the real-time voting system.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/dotp_test.png" alt="DOTP Layer Cake aesthetic" /></p>

<p>Due to a lucky coincidence during a debugging session I discovered that the latest version of <em>Chrome</em> was able to apply transparency to embedded <em>Flash</em> movies. This meant that I&#8217;d be able to play back all the channels at once, which would make for a more dream-like aesthetic and perhaps even increase the level of immersion.</p>

<h3>Reception &amp; Further development</h3>

<p>It worked out quite well in the end. At least for a two week project. Simon didn&#8217;t seem very impressed, but the crowd loved it.</p>

<p>In the version we presented it was already possible to create new channels on the fly by calling a function from the browser&#8217;s console. That as well as the remote design could do with some love in my opinion. Ideally you also wouldn&#8217;t have to restart the server every time you wanted to have a session, rather you&#8217;d create a theatre of your own using a unique id so you could invite people on the internet to join. Alas, I am fed up with <em>WebSockets</em> and I&#8217;ll gladly hand such tasks to all you enthusiastic tinkerers out there who want to learn and or simply point out what I&#8217;ve been doing wrong (I know for instance that the way I bundled socket.io with the app was completely off). You can find the repository on my <a href="https://github.com/bloomingbridges/YouMote">GitHub</a>. A short video demonstration can be found on my <a href="http://www.youtube.com/watch?v=nNVCAYqJ_BE">YouTube</a> channel.</p>

<p>To this day I am still not entirely sure whether the struggle with <em>Node</em> was actually worth it. Expecially the package management causes a LOT of confusion in combination with hosting services and writing server-side <em>JavaScript</em> - albeit its familiarity - still seems blatantly pointless.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/dotp_snap1.png" alt="DOTP Screenshot" /></p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/dotp_snap2.png" alt="DOTP Screenshot" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Re-visiting blender]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/re-visiting-blender/"/>
    <updated>2012-11-03T13:24:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/re-visiting-blender</id>
    <content type="html"><![CDATA[<p>There comes a point in a hacker slash designer&#8217;s career where one realises that they haven&#8217;t touched 3D in a while. For me that point was during placement at <em>Refunk</em>, when I was doing research on the emerging <em>WebGL</em> standard and tried creating my geometry in <em><a href="http://mrdoob.com">Ricardo Cabello</a></em>&#8217;s <em>three.js</em> in code, one vertex at a time.</p>

<p>Perhaps the most formative experience I had in relation to three-dimensional computer graphics was listening to <em>Marcin Ignac</em>&#8217;s talk at <em>FITC Amsterdam</em>. His approach of making his own tools using Mac OS X technologies and his <a href="http://marcinignac.com/projects/cindermedusae/">Cindermedusae</a> project in particular refueled my passion for the field and even taught me some valuable and transferable things about programmatic animation in general.</p>

<p>Now as it often is the case you find yourself very motivated to create something when leaving a conference like that, so for the remaining duration of my time away from uni I set myself the task of re-creating my <em>blooming bridges over withering waters visualisation</em> in mind-blowing 3D using WebGL.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/spaceworms.png" alt="WebGL version of bloomingbridges.co.uk prototype" /></p>

<p>What I didn&#8217;t consider was - that as with learning any craft - it would take up a considerate amount of my time and spare time is a luxury item. A piece of software was needed that would allow me to speed up the process of &#8220;building&#8221; objects for my scene.</p>

<p>The preferred choice of 3D package in the office was <em>Cinema4D</em>, but what I had in my mind did not require a professional high-end solution to execute, I was only after little visual feedback. Being in the Netherlands and having only read positive things about <em>blender</em>&#8217;s recent interface makeover and <em>Cycles</em> rendering engine I felt intrigued to give it another chance after I had dismissed it so passionately in my second year of uni.</p>

<p>It wasn&#8217;t the most glorious idea I&#8217;ve ever had, but I couldn&#8217;t imagine finding the stamina to tame the temperemental behemoth that is <em>Maya</em> some more and learning an entirely new piece of software felt counter-productive. What&#8217;s there to say about the new <em>blender</em> that doesn&#8217;t make me loose myself in a fit of rage. Once again it is exemplary for my love-hate-relationship with <em>open-source</em> software:</p>

<p>The new UI looks definitely more pleaasing to the eye and less broken, but fitted within an unreliable window layout and decorated with new secondary panels it&#8217;s not inherently more efficient to use. The old key mapping has been scrapped as well. Where you once triggered a contextual menu for adding primitives to your scene by tapping the space bar, you are now greeted with a useless little help window. Not enough that a great deal of <em>blender</em>&#8217;s functionality still isn&#8217;t exposed in the interface, the options available still don&#8217;t give you an indication as to what key they respond to.</p>

<p>I can see the value of customisation in productivity software, but for the love of God is it too much to ask for to agree on a standard that just works? Also, whoever thought that integrating trackpad gestures into the software without any consideration for the consequences (it renders the 3D view literally unusable without an external mouse or graphics tablet) might want to contemplate a change of career.</p>

<p>The new rendering engine on the other hand looks quite promising. <em>Cycles</em> is a node-based renderer with <em>global illumination</em> (objects emitting light etc.) at its core for more realistic-looking results. Once again, the sporadic documentation makes it more difficult to pick up if you don&#8217;t have a load of time on your hands to experiment.</p>

<p>In conclusion I can say that re-learning <em>blender</em> was a daunting task, yet considering the free options out there presents just one of the many challenges that I feel I just had to master (I&#8217;m still hoping to be able to add <em>OpenFrameworks</em>, Max/MSP and OpenGL to my toolchain one day).</p>

<p><img src="https://dl.dropbox.com/u/998319/lowpolyhearts.jpg" alt="Low-poly hearts inspired by Helena Coard" />
An artwork inspired by Plymouth-based Illustrator <a href="http://www.helena-coard.co.uk">Helena Coard</a></p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/lilyfinal.png" alt="bloomingbridges v6 logo" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FYP: Aesthetics]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/fyp-aesthetics/"/>
    <updated>2012-11-03T13:18:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/fyp-aesthetics</id>
    <content type="html"><![CDATA[<p>What follows is a set of images that have influenced my aesthetic vision of <strong>IN//CUBUS</strong>, a moodboard if you will.</p>

<h3>Existing installations</h3>

<p><img src="https://dl.dropbox.com/u/998319/DAT/FYP/fearful_symmetry.jpg" alt="Fearful Symmetry" /></p>

<p><a href="http://www.bbc.co.uk/news/technology-19357951">Ruairi Glynn&#8217;s robot puppetry piece &#8216;Fearful Symmetry&#8217; on BBC</a></p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/FYP/UFO.jpg" alt="UFO by Ross Lovegrove" /></p>

<p><a href="http://rcruzniemiec.tumblr.com/post/32942339132/ufo-ross-lovegrove-the-visitors-and-travellers">UFO by Ross Lovegrove</a></p>

<h3>Materials</h3>

<p><img src="https://dl.dropbox.com/u/998319/DAT/FYP/OrangeSphere1967CastPolyester.jpg" alt="" />
<img src="https://dl.dropbox.com/u/998319/DAT/FYP/black_mass.jpg" alt="" />
<img src="https://dl.dropbox.com/u/998319/DAT/FYP/ethrnorr.gif" alt="" />
<img src="https://dl.dropbox.com/u/998319/DAT/FYP/cryptic.jpg" alt="" /></p>

<h3>Interior</h3>

<p><img src="https://dl.dropbox.com/u/998319/DAT/FYP/gba-006.jpg" alt="" />
<img src="https://dl.dropbox.com/u/998319/DAT/FYP/led_cubes.jpg" alt="" />
<img src="https://dl.dropbox.com/u/998319/DAT/FYP/17.jpg" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The blogjectosphere]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/the-blogjectosphere/"/>
    <updated>2012-11-03T13:10:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/the-blogjectosphere</id>
    <content type="html"><![CDATA[<p>In our first <em>Everyware</em> seminar we tried to group and make sense of all the new terminology that revolves around the <em>Internet of Things</em>. Certainly too big an endeavour as most of the names for networked objects were borrowed from science-fiction and there isn&#8217;t just one author with one view of the world.</p>

<p>One of the more interesting naming conventions is that of a &#8220;blogject&#8221;, an object that blogs. While it may sound somewhat disrespectful to journalism to call a stream of logged sensor values a &#8220;blog&#8221;, it fulfills the same purpose: to stir up conversation. It doesn&#8217;t matter whether your blogject is in fact a coconut measuring noise pollution or a pigeon reporting on a city&#8217;s air quality, their existence is always justified in that they allow us to learn about things we normally would not be able to (in case of the pigeons) or in that we place them in the environment as part of our political agenda.</p>

<p>Once a thing is considered useful it becomes more than a lifeless object, it becomes part of our society.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Seminar rundown: Synesthesia]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/seminar-rundown-synesthesia/"/>
    <updated>2012-10-29T23:05:00+00:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/seminar-rundown-synesthesia</id>
    <content type="html"><![CDATA[<p>One of <em>DAT</em> lecturers&#8217; all-time favourites found its way into our AudioVisualogy seminar discussion, synesthesia. Much to my surprise I learnt a little later that I knew someone in my inner circle of friends, who is affected by this rare condition so I thought I&#8217;d ring her up and document my findings, like all the serious researchers do.</p>

<p>My friend claims to have been a synesthete for all of her life, naturally she assumed that everybody else was seeing the world in the same manner as herself. As opposed to <em>Oliver Sacks&#8217;s</em> research on subjects wo associate colours with music she only &#8220;suffers&#8221; from the following:</p>

<ul>
<li>letters</li>
<li>words (a gradient of colours of the individual letters)</li>
<li>numbers (and ranges of number i.e. dates)</li>
<li>days of the week</li>
</ul>


<p>When looking at words there are some characters that are more dominant than others, especially the first letter, e.g. the name <em>Flo</em> looks primarily purple to her. Interestingly the &#8216;o&#8217; is changing colour depending on the context, which she does not have an explanation for. Here goes the chart:</p>

<pre><code>A - dark red
B - light blue
C - grey
D - light green
E - green
F - purple
G - organe
H - grey
I - dark purple (undefined)
J - orange / brown
K - purple
L - dark purple
M - red
N - pink
O - changing colours, alternating between orange and blue
P - dark blue
Q - grey to white
R - green
S - yellow
T - green
U - orange
V - grey
W - orange (with a little brown)
X - dark grey
Y - dark grey also
Z - white

?   - yellow
!., - mostly grey-ish
()  = more brown
ß   - white
の  - pink

Monday - blue / purple
Tuesday - green
Wednesday - pink
Thursday - dark green
Friday - orange
Saturday - Yellow
Sunday - orange
</code></pre>

<p>It gets more interesting when looking at special characters, in particular characters from foreign alphabets like cyrillic or the Japanese hiragana writing system. When I presented her the written Greek alphabet she saw colours due to the fact that they all came with annotations in Latin script. However, when I showed her a random Japanese sentence they all looked the same to her, up until I taught her what the characters sounded like.</p>

<p>One possible explanation colour association for the letter-based synesthesia was based on a poster of the alphabet when she was learning to read. Why to her the alphabet reads backwards however, is still a mystery. Also interesting when looking at numbers, the further you go down into the negative the darker the colours become.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Birds and XBees - The immediate future of Birdwire]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/birds-and-xbees-the-immediate-future-of-birdwire/"/>
    <updated>2012-10-22T19:36:00+01:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/birds-and-xbees-the-immediate-future-of-birdwire</id>
    <content type="html"><![CDATA[<p>I&#8217;m pleased to report that the <em>Birdwire</em> presentation went rather well. Lee caught enough interest to approach us after the session and ask us whether we&#8217;d like to put one out in the open. Of course we agreed to take the project further - if there was time - so we agreed to meet up the coming Monday so Lee could provide us with some more equipment needed to make the birdhouse self-sustainable and wireless.</p>

<p>Somehow we didn&#8217;t quite manage to meet up so the future of Birdwire is a little cloudy at the moment. In the meantime I got the chance to play with XBees for our <em>Everyware</em> project, so the initial learning curve would be marginal. Due to the planned XBee incorporation the upload of the code to GitHub has also been put on hold for the time being, for which I apologise.</p>

<p><strong>EDIT 08.12.2012</strong></p>

<p>It&#8217;s fair to assume that Lee&#8217;s initial interest has vanished by now as he still hasn&#8217;t found the time to respond to my emails. In addition to our growing disappointment we noticed that the once so promising piece of woodwork that Anish contributed to the project is now being used as a mere doorstopper, but it doesn&#8217;t end here.</p>

<p>As friends and the internet pointed out to us, there have been scarily similar developments in other corners of the world almost simultaneously: Latvian artist <em>Voldemars Dudums</em>&#8217;s <a href="http://designtaxi.com/news/354532/Real-Birds-Post-Tweets-On-Twitter">Hungry Birds</a> project for one and David Bowen&#8217;s <a href="http://dwbowen.com/fly_tweet.html">Fly Tweet</a> explore pretty much the same concept.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/fly_tweet.jpg" alt="fly tweet" /></p>

<p>Going through the amounts of <em>facebook likes</em> comparable projects have amassed doesn&#8217;t faze me one bit, although it begs the question whether our idea of presenting ideas to the outside world is fundamentally flawed (We usually assume that all work is done once we created a project website and pushed it onto a web server). While working in groups and according to briefs never gives you that feeling of authorship that makes being an artist so rewarding I draw satisfaction from knowing that I&#8217;ve finally started to think like one.</p>

<p>At the end of the day I can still pride myself for having incepted the animal twitter client with the least visible interface. No keyboards were harmed during the development of this project :></p>

<p>Finally, I have found the time to prepare the birdwire codebase for the general public and immortalised it with its own <a href="https://github.com/bloomingbridges/Birdwire">GitHub repository</a> (for licencing reasons it doesn&#8217;t contain the font I&#8217;ve used in the Processing sketch, <em>Avenir Light</em>). Feel free to improve on it, maybe together we can convert birdwatchers into birdreaders and inspire many more bird-related projects to come.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The ultimate Shakespearean]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/the-ultimate-shakespearean/"/>
    <updated>2012-10-21T19:58:00+01:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/the-ultimate-shakespearean</id>
    <content type="html"><![CDATA[<p>At the start of this year I rejoined the <em>Amateur Dramatics</em> society. When the new, more ambitious committee were asking us to submit our own plays for the upcoming christmas showcase, the following ran through my head:</p>

<p>Since I am not anywhere decent at writing I thought I could let my computer write a play more me generating phrases and characters at random. This thought was hugely influenced by my recent experiments with the <a href="http://www.rednoise.org/rita/">RiTa</a> library for Processing, which has made its way into <a href="http://benashman.co.uk/p/birdwire">Birdwire</a> app.</p>

<p>However, since this approach would likely result in complete nonsense, I&#8217;d like to suggest the more sensible <em>Dadaist</em> way: I&#8217;d collect a few public domain plays and have a script pick out phrases and stage instructions and assign them to a fixed amount of characters. Essentially collaging literature, or perhaps even recycling.</p>

<p>The resulting play probably wouldn&#8217;t make the cut, yet I&#8217;m really eager to see one version acted out professionally as this would present quite the challenge to any actor regardless their skill level. Maybe by the time Easter comes along I will have one ready.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/shakespeare.jpg" alt="Jane Boyer - The complete work of William Shakespeare" /></p>

<p><em><strong>The complete work of William Shakespeare</strong> - &copy; 2011 <a href="http://janeboyer.com">Jane Boyer</a></em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Classroom gnomes]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/classroom-gnomes/"/>
    <updated>2012-10-15T21:51:00+01:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/classroom-gnomes</id>
    <content type="html"><![CDATA[<p>So how does one go about teaching kids ecological responsibility? First of all we need ways of measuring their wastefulness and I couldn&#8217;t think of an application more appropriate for the <em>XBee</em> shields that I so eagerly want to get my hands on.</p>

<p>In combination with your usual suspects of sensors (temperature, light etc.) we could then send all our collected data wirelessly to a central location, the pc that will sit inside the new learning space at the other end of the campus, essentially creating an <a href="http://www.i-dat.org/eco-os-workshop-ecoid-prototype/">Ecoid</a> network of our own.</p>

<p>To make the whole process a little easier to comprehend I believe we shouldn&#8217;t just hide the sensors away in the building, but should create some interesting designs for their housing instead. Think garden gnomes for a second. They all visibly wear their tool of choice, which associates them with a certain task. You never see them moving, but if they&#8217;re doing their job you know they must have communicated with each other when you weren&#8217;t looking.</p>

<p>This thinking inspired the designs for the wireless sensors to be put in their class- and common rooms. The heat sensor for instance would be hugging the radiator and and the wind sensor would be sat on a windowsill somewhere.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/devonport_gnomes.png" alt="Gnome designs" /></p>

<p>The data collected will be analysed and visualised in two different ways. A literal visualisation in the form of bar charts and graphs will mainly be of interest to the world outside of the campus, mainly to show off how energy conscious the <em>Devonport High School for Boys</em> has become.</p>

<p>The visualisation that goes inside the learning space is of a more experimental nature. It will invite pupils to play with it and react to their actions in real time. Further will there be a console cut from Acrylic, which takes bricks as inputs to turn certain aspects of the visualisation on or off.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/element_stones.jpg" alt="Four Elements" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hold the Line]]></title>
    <link href="http://bloomingbridges.github.com/Sammelsurium/hold-the-line/"/>
    <updated>2012-10-15T01:01:00+01:00</updated>
    <id>http://bloomingbridges.github.com/Sammelsurium/hold-the-line</id>
    <content type="html"><![CDATA[<p>My former housemate Rafael used to present me with his latest acquisitions that ranged from retro gaming hardware to iPhone accessories on a weekly basis. The least useful one, a phone handset that plugs into the bi-directional headphone jack of an iPhone, re-entered my trail of thoughts recently as I was thinking about gallery spaces.</p>

<p><img src="https://dl.dropbox.com/u/998319/DAT/Smoke.jpg" alt="iPhone handset accessory" /></p>

<p>It certainly holds a certain novelty to attach a phone to a phone, but when plugged into a computer for instance it becomes an interface rather than an unwieldy extension. With the help of a sound shield one could have an Arduino play holding or elevator music for eternity through the handset whenever it&#8217;s picked up. With the help of capacitive sensors one could measure the tolerance levels of gallery viewers and visualise them on a nearby wall in one way or other.</p>

<p>I am not taking this anywhere, this entry simply marks the beginning of a series of blog posts where I jot down the more shareworthy ideas that cross my mind.</p>
]]></content>
  </entry>
  
</feed>
